# ğŸ§­ AI-Safety-Journey

_Exploring AI Safety, Interpretability, Evaluation, and Sustainability_

This repo documents my personal journey transitioning into **AI Safety research**. It combines practical coding exercises, interpretability experiments, and sustainability analysis â€” with the goal of building a strong, research-ready portfolio. 

## ğŸ¯ Objectives
- Deepen my understanding of **AI Safety**: interpretability, alignment, evaluation, and sustainability.  
- Build hands-on experience with **modern ML frameworks** (PyTorch, Transformers).  
- Explore **explainability** and **robustness** techniques for model understanding.  
- Quantify **sustainability metrics** (energy efficiency, COâ‚‚ emissions).  
- Develop a **public GitHub portfolio** to support PhD or research applications.

## ğŸ§± Repository Structure
AI-Safety-Journey/

- *Fastai*: Experimentation with Fastai library
- *InspectAI*: Experimentation with InspectAI library
- *notebooks*: Jupyter notebooks (experiments, visualizations)
- *scripts*: Reusable code and functions
- *data*: Datasets (ignored by git)
- *reports*: Written summaries, reflections, and analysis
- *requirements.txt*: Dependencies

## âš™ï¸ Environment Setup
1ï¸âƒ£ Clone this repository
    git clone https://github.com/<YOUR_USERNAME>/AI-Safety-Journey.git
    cd AI-Safety-Journey

2ï¸âƒ£ Create a virtual environment
    python -m venv venv
    source venv/bin/activate   # macOS/Linux
    .\venv\Scripts\activate    # Windows

3ï¸âƒ£ Install dependencies
    pip install -r requirements.txt

4ï¸âƒ£ Add your secret API keys
Copy .env.example â†’ .env and fill in:
    OPENAI_API_KEY=your_openai_key_here
    HUGGINGFACE_HUB_TOKEN=your_hf_token_here

## ğŸ§  Coding Tasks & Descriptions

### Weekâ€¯1 â€“ Foundations
1. notebooks/week1_fine_tune_model.ipynbâ€ƒFineâ€‘tune a small Transformer on a text classification dataset. 
2. scripts/data_pipeline.pyâ€ƒCreate a simple dataâ€‘loader pipeline.  
3. reports/foundations_summary.mdâ€ƒSummarize core AIâ€‘safety challenges.  

### Weekâ€¯2 â€“ Interpretability
1. notebooks/week2_shap_lime_experiments.ipynbâ€ƒApply SHAPâ€¯/â€¯LIME.  
notebooks/week2_attention_visualization.
2. ipynbâ€ƒVisualize Transformer attention.  
3. reports/interpretability_reflection.mdâ€ƒ
4. Explain how interpretability improves transparency.  

### Weekâ€¯3 â€“ Evaluationâ€¯&â€¯Sustainability
notebooks/week3_model_comparison.ipynbâ€ƒCompare model accuracy and efficiency.  
notebooks/week3_energy_tracking.ipynbâ€ƒTrack energyâ€¯+â€¯COâ‚‚ usage.  
reports/sustainability_analysis.mdâ€ƒDocument tradeâ€‘offs between performanceâ€¯&â€¯energy use.  

### Weekâ€¯4 â€“ Capstone
notebooks/week4_project.ipynbâ€ƒCombine interpretabilityâ€¯+â€¯sustainability analysis.  
reports/final_reflection.mdâ€ƒSummarize lessons learned.  
README_update.mdâ€ƒUpdate project summary with results.  

## ğŸ§° Key Libraries
torch, transformers, datasets, evaluate, scikitâ€‘learn, shap, lime, captum, matplotlib, seaborn, codecarbon, experimentâ€‘impactâ€‘tracker, psutil, wandb, mlflow, jupyter, tqdm, numpy, pandas.

## ğŸ”’ APIâ€¯Key Management
Keep credentials out of git.  
Add to localâ€¯.env:  
    OPENAI_API_KEY=skâ€‘xxxx  
    HUGGINGFACE_HUB_TOKEN=hfâ€‘xxxx  
Load in code:  
    from dotenv import load_dotenv; load_dotenv(); key=os.getenv("OPENAI_API_KEY")  
Ensure `.env` is listed in `.gitignore`.

## ğŸ§© Future Work
- Test multimodal interpretability  
- Evaluate safety alignment metrics  
- Study RLHF methods  
- Write summary blog post

## ğŸ“š Key References
Amodeiâ€¯etâ€¯al.â€¯(2016)â€¯â€“â€¯*Concreteâ€¯Problemsâ€¯inâ€¯AIâ€¯Safety*  
Olahâ€¯etâ€¯al.â€¯(2018)â€¯â€“â€¯*Buildingâ€¯Blocksâ€¯ofâ€¯Interpretability*  
Schwartzâ€¯etâ€¯al.â€¯(2019)â€¯â€“â€¯*Greenâ€¯AI*  
Russellâ€¯(2019)â€¯â€“â€¯*Humanâ€¯Compatible*  

## ğŸ‘¤ Author
**Matt Gillie** â€“ AIâ€¯Safetyâ€¯&â€¯Machineâ€¯Learningâ€¯Enthusiast  
ğŸ’¼â€¯[LinkedIn](https://www.linkedin.com/in/matt-gillie-48375a10b/)â€ƒğŸ§ â€¯Focus:â€¯Interpretabilityâ€¯â€¢â€¯Evaluationâ€¯â€¢â€¯Sustainableâ€¯AI  
